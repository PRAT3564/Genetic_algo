{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deap\n",
      "  Downloading deap-1.4.1-cp39-cp39-win_amd64.whl (109 kB)\n",
      "     ------------------------------------ 109.9/109.9 kB 491.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\pratham\\anaconda3\\lib\\site-packages (from deap) (1.24.3)\n",
      "Installing collected packages: deap\n",
      "Successfully installed deap-1.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install deap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (Neurons per layer, Number of layers): [18, 71]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Placeholder evaluation function (replace this with your actual implementation)\n",
    "def evaluate(individual):\n",
    "    # Your actual evaluation logic here (e.g., train a neural network and compute fitness)\n",
    "    # This example returns a random fitness value for demonstration purposes\n",
    "    return random.random(),  # Note: Fitness values must be returned as a tuple\n",
    "\n",
    "# Genetic algorithm parameters\n",
    "POPULATION_SIZE = 10\n",
    "GENERATIONS = 5\n",
    "\n",
    "# Create types for fitness and individuals\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,))  # Minimize fitness\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMin)\n",
    "\n",
    "# Initialize toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Define attributes and individuals\n",
    "toolbox.register(\"attr_neurons\", random.randint, 10, 100)  # Number of neurons\n",
    "toolbox.register(\"attr_layers\", random.randint, 2, 5)      # Number of layers\n",
    "toolbox.register(\"individual\", tools.initCycle, creator.Individual,\n",
    "                 (toolbox.attr_neurons, toolbox.attr_layers), n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "# Genetic operators\n",
    "toolbox.register(\"evaluate\", evaluate)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutUniformInt, low=10, up=100, indpb=0.2)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "# Create initial population\n",
    "population = toolbox.population(n=POPULATION_SIZE)\n",
    "\n",
    "# Run the genetic algorithm\n",
    "for gen in range(GENERATIONS):\n",
    "    offspring = algorithms.varAnd(population, toolbox, cxpb=0.5, mutpb=0.1)\n",
    "\n",
    "    # Evaluate the offspring\n",
    "    fitnesses = map(toolbox.evaluate, offspring)\n",
    "    for ind, fit in zip(offspring, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    # Select the next generation\n",
    "    population = toolbox.select(offspring, k=len(population))\n",
    "\n",
    "# Get the best individual and parameters\n",
    "best_individual = tools.selBest(population, k=1)[0]\n",
    "best_params = best_individual\n",
    "\n",
    "print(\"Best Parameters (Neurons per layer, Number of layers):\", best_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitness Value: (0.14053872812496473,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load your actual data (replace this with your data loading logic)\n",
    "def load_data():\n",
    "    X = np.random.rand(100, 5)  # Example: Random feature matrix\n",
    "    y = np.random.rand(100)      # Example: Random target vector\n",
    "    return X, y\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(individual):\n",
    "    X, y = load_data()  # Load your data\n",
    "\n",
    "    # Extract parameters from the individual\n",
    "    hidden_layer_sizes = (individual[0],) * individual[1]  # Example: (10, 10, 10) for 3 layers with 10 neurons each\n",
    "    learning_rate_init = individual[2]                     # Example: Learning rate\n",
    "    momentum = individual[3]                               # Example: Momentum\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize and train neural network\n",
    "    nn = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                      learning_rate_init=learning_rate_init,\n",
    "                      momentum=momentum,\n",
    "                      max_iter=500)  # Adjust max_iter as needed\n",
    "    nn.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate performance on validation set (test set)\n",
    "    y_pred = nn.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # Return fitness value (minimize mean squared error)\n",
    "    return mse,\n",
    "\n",
    "# Example usage of the evaluate function\n",
    "# Replace this with your actual implementation\n",
    "# Example individual representing (number of neurons, number of layers, learning rate, momentum)\n",
    "individual = [10, 3, 0.01, 0.9]  # Example individual\n",
    "fitness_value = evaluate(individual)\n",
    "print(\"Fitness Value:\", fitness_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitness value: 3024.391025999067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pratham\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load and preprocess the dataset (example with the diabetes dataset)\n",
    "def load_data():\n",
    "    diabetes = load_diabetes()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(individual):\n",
    "    # Extract parameters\n",
    "    hidden_layer_sizes = tuple(individual[0:2])\n",
    "    learning_rate_init = individual[2]\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    X_train, X_test, y_train, y_test = load_data()\n",
    "    \n",
    "    # Initialize and train neural network\n",
    "    nn = MLPRegressor(hidden_layer_sizes=hidden_layer_sizes,\n",
    "                      learning_rate_init=learning_rate_init,\n",
    "                      max_iter=1000)  # Adjust max_iter as needed\n",
    "    nn.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate performance on validation set\n",
    "    y_pred = nn.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # Return fitness value (minimize MSE)\n",
    "    return mse,\n",
    "\n",
    "# Example usage:\n",
    "individual = [10, 20, 0.001]  # Example individual (neurons in hidden layers, learning rate)\n",
    "fitness_value = evaluate(individual)\n",
    "print(\"Fitness value:\", fitness_value[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
